/************************
Threads and Concurrency :
************************/

Definition : 
Sequence of execution with its own context, operating within a shared address space.

Characteristics :
-active entity, executing unit of work on behalf of a process
-threads may work simultaneously
-threads require co-ordination to resources (processors, I/O devices)

Process vs Thread :
A single threaded process is represented by its PCB (code, data, files, virtual address space physical 
mapping, context -> speial registers)

Threads represnet multiple independent execution contexts in a shared address space. This means each thread 
has its own IR, PC, SP, stack. The OS represents a multi-threaded with a more complex PCB which holds shared 
information (mapping, code, data, files) and individual contexts (stack, special register values).

Why are threads useful : 
-threads can execute the same code on different portions of input simultaneously
-threads may have different functions, allowing priority management 
-when each thread is on a different processor, they each have their own cache and smaller instruction set, 
 therefore hotter cache
-multi-process applications use more memory compare to multi-threaded
-also inter thread-communication (shared memory) is generally cheaper than inter-process communication
 (recall even share memory IPC has different mapping overheads)

Are threads useful when there are more threads than processors :
If a thread T1 is waiting on I/O (e.g. disk) it may make sense to context switch to another thread. Note 
that thread context switches are cheaper than process  swithces because threads don't need a new address 
mapping.

Basic thread mechanisms : 
-thread data structure
	-> indentify threads, track resource usage
-creation and management mechanisms
-co-ordinate concurrent threads which share an address space 

Concurrency : 
Concurrent processes operate within their own address space (isolation, protection). 
Threads may tamper with each other's shared data. Data race occurs when simultaneous access occurs.

Thread synchronization mechanisms : 
Mutual exclusion -> lets one thread at a time access data, i.e. mutex
Condition variables -> waiting on conditions allows conditional processing

*** general concepts in thread libraries *** 
Threads and Thread Creation : 
-Thread type 
	-> data structure describing thread (thread ID, special registers, stack, attributes)
-Fork(proc, args)
	-> create a thread (with a new thread data structure, starting instruction at proc with args)
	-> not unix fork 
-Join(thread)
	-> parent blocked until child completes
	-> terminate a thread and free thread data structures
	-> returns child computation result to parent

// pseudo code
Thread thread1;
SharedList list;
thread1 = fork(safeInsert, 4); // called by thread0
safeInsert(6); // order of insertion is unclear given this code
join(thread1); // parent blocked until child finishes

// above, there is a danger of data race
// data could be overwritten by simultaneous access, overwriting for example one of the two values
1) Mutexes : 
A lock that should be used when accessing shared data. A thread locks a mutex, getting exclusive access to 
the shared resource. Other threads are blocked from accessing it. Mutex data structure should contain 
resource status, list of blocked threads, owner (who has the lock).

// like monitor in Java?
// common API's have lock(m) and unlock(m) functions
Lock(mutex) {
	// code in the lock operation block is the critical section
	// only one thread may execute this code at a time
	// other threads trying to lock a mutex are out in a blocked list (sometimes a simple queue)
} // implicitly frees mutex (these semantics are sprcific to Birell's paper)

Mutex example : 
list<int> my_list;
Mutex m;
void safe_insert(int i){
	Lock(m){
		my_list.insert(i);
	}
	// unlock
}

2) Producer / Consumer :
What if mutual exclusion processing should only occur under certain conditions?
Many producer threads, one consumer thread. The consumer must not read when the list is empty and the producer must not 
produce while the list is full. Customization : the consumer reads when the buffer is full.

// pseudocode (wasteful implementation)
// main thread creates producer and consumer threads 
for i=0..10
	producers[i] = fork(safe_insert, NULL) // create all producers
consumer = fork(print_and_clear, my_list) // create one consumer  

// producers : safe insert
Lock(m){
	list->insert(my_thread_id)
} // unlock

// consumer : print_and_clear
Lock(m){
	// wasteful approach!
	// therefore use condition variables to control thread behaviour
	if my_list.full -> print_all(); clear all elements from list
	else -> release lock resume polling
} 
// unlock

/************
modified condition variable code
************/

//consumer : print_and_clear
Lock(m){
	while(my_list.not_full())
		Wait(m, list_full); // waits on a particular mutex for a particular condition
	my_list.print_and_remove_all();
}
// unlock

// producer : safe_insert
Lock(m){
	my_list.insert(my_thread_id)
	if my_list.full()
		Signal(list_full); // signals all the threads waiting for a particular signal
}
// unlock

Conditional Variable API :
-Condition type (?) 
-Wait(mutex, condition)
	-> mutex automatically released on entering and acquired on leaving Wait() due to a Signal()
	-> condition which triggers acquiring the mutex the thread is waiting for 
-Signal(condition)
	-> notify one thread which is waiting on this condition 
-Broadcast(condition)
	->notify all threads which are waiting on this condition

Condition variable : 
-mutex reference // in this pseudocode, the mutex object is passed in Wait(mutex, condition)
-collection of waiting threads // select thread from this collection to signal or notify

// E.g.
Wait(mutex, condition) {
	// release mutex
	// go to wait queue
	// ...wait...
	// remove from queue
	// acquire mutex
}

Reader/Write example : 
Multiple reader threads and one writer thread for a shared file. Reader cannot read while a writer is writing, but 
multiple readers can read simultaneously.

Mutex counter_mutex; 
Condition read_phase;
Condition write_phase; 
int resource_counter = 0; // semaphore tracks the condition at which a signal should be sent

// 1. multiple readers 
Lock(counter_mutex){
	while(resource_counter == -1)
		wait(counter_mutex, read_phase);
	resource_counter++;
} // unlock
// ...read data... (CRITICAL SECTION)
Lock(counter_mutex){
	resource_counter--;
	if(readers == 0)
		Signal(write_phase);
} // unlock

// 2. one writer
Lock(counter_mutex){
	while(resource_counter != 0)
		Wait(counter_mutex. write_phase);
	resource_counter = -1;
} // unlock
// ...write data... (CRITICAL SECTION)
Lock(counter_mutex){
	resource_counter = 0;
	Broadcast(read_phase);
	Signal(write_phase);
} // unlock

Critical Section Structure : 

Lock(mutex){
	while(!predicate_indicating_access_ok)
		wait(mutex, cond_var)
	update state => update predicate
	signal and/or broadcast(cond_var_with_correct_waiting_threads)
} // unlock

Critical structure with proxy variable : 
Reaps benefits of Mutexes while allowing multiple threads to read the resource. This implements a more complex sharing 
scenario.

// ENTER CRITICAL SECTION
perform critial operation (read / write)
// EXIT CRITICAL SECTION

//ENTER 
Lock(mutex){
	while(!predicate_indicating_access_ok)
		wait(mutex, cond_var)
	update predicate
} // unlock

// EXECUTE CRITICAL SECTION CODE

//EXIT
Lock(mutex){
	while(!predicate_indicating_access_ok)
		wait(mutex, cond_var)
	signal/broadcast(cond_var)
} // unlock

Common Pitfalls : 
-keep track of mutex/condition variables used with a resource
	-> e.g. mutex_type m1; 
-check that you are using lock and unlock correctly
	-> e.g. Did you forget one? Compilers may generate lock warnings.
-make sure to always use a single mutex to access a single resource
-check that you are signalling/broadcasting the corect condition	
-check that you use broadcast vs signal when you need to wakeup multiple threads
-do you need priority guarantees

Spurious wakeups :
// todo : learn more

Deadlock : 
Definition -> Two or more competing threads wait on each other to complete, since multiple resources are required, but 
neither ever finish.
Solution -> 
(Prevention) Enforce lock order.
(Detection and recovery) Rollback on lock.
(Apply ostrich algorithm) Do nothing, think ostrich with head in the sand.

Kernel level threads : 
Means OS is multithreaded, visible to and managed by kernel.
OS level services or support for user processes/threads.

User level threads :
User thread must be related to a kernel level thread, then the Kernel must dispatch the kernel thread.

Multithreading models : 

1) One to one model
-Each user thread is linked to a kernel thread.
Pros -> OS sees/understands user threads, synchronization, blocking...
Cons 
-> Every operation requires crossing to kernel space (system call)
-> OS may have limits on policies, thread number, portability

2) Many to one model :
-All user level threads mapped to a kernel thread
-User level library decides how to map user threads to kernel threads
Pros -> Portable, doesn't depend on OS limits/policies because shared library handles thread mapping.
Cons  
-> OS has no insights into application needs
-> One danger is the OS may block entire process if one use thread blocks on I/O

3) Many to many model : 
Pros -> Best of both worlds
Cons -> Requires some co-ordination between user and kernel thread managers

Scope of Multithreading : 

System scope 
-> system wide thread management supported by OS thread managers (e.g. CPU scheduler)

User scope 
-> user library manages threads within a process

Multi-threading Patterns

1) Boss/Worker Pattern
-boss assigns work to workers 
-worker performs entire task
	->workers don't need to synchronized
	->boss is the slow point 
-one option is to use a queue worker (producer/consumer)
	-> boss doesn't need to track workers anymore
	-> queue synchronization needed

How many workers?
-add on demand (can be inefficient) *** Apache *** 	
-maintain thread pool *** Node.js *** 	
-static vs dynamic

Pro 
-> Simple to implement

Con 
-> Thread pool management overhead. 
-> Doesn't track locality, i.e. workers are more efficient at performing the same task (cache). 

1.5) Boss/Worker variants
-all workers equal vs specialized workers
	-> more boss overhead for sorting tasks, offset by better thread efficiency
	-> i.e. this technique exploits locality leading to hotter cache
	-> harder load balancing
	
2) Pipelined Pattern
-thread assigned one subtask
-entire tasks is a sequence of stages, each executed by a different thread
-i.e. assembly line
-throughput determined by slowest stage
	-> address this by maintaining thread pools per stage to increase slower stages
	-> shared buffer communication between stages

Pros -> specialization and locality
Cons -> balancing and synchronization overheads
	
3) Layered Pattern
-each layer performs a group of related tasks 
-end to end  task must pass through all laters

Pros -> Specialization and locality, less fine grained
Cons -> Not suitable for all applications, synchronization




